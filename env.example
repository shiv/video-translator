# =============================================================================
# AI Video Translation Service - Environment Configuration
# =============================================================================
# Copy this file to .env and update with your actual values

# =============================================================================
# REQUIRED CONFIGURATION
# =============================================================================

# Hugging Face API token for AI models
# Get your token from: https://huggingface.co/settings/tokens
# You need to accept user agreements for segmentation and diarization models
HUGGING_FACE_TOKEN=hf_your_token_here
# Alternative variable name (same as above)
# HF_TOKEN=hf_your_token_here

# =============================================================================
# CORE APPLICATION SETTINGS
# =============================================================================

# Server Configuration
HOST=0.0.0.0
PORT=8000

# Storage Directories
OUTPUT_DIRECTORY=output/
UPLOAD_DIRECTORY=uploads/

# Logging Configuration
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# =============================================================================
# JOB PROCESSING CONFIGURATION
# =============================================================================

# Maximum number of concurrent video processing jobs
MAX_CONCURRENT_JOBS=2

# File upload limits
# Maximum file size in megabytes
MAX_FILE_SIZE_MB=200

# WebSocket Configuration
# Ping interval for WebSocket connections (seconds)
WEBSOCKET_PING_INTERVAL=30

# =============================================================================
# AI MODEL CONFIGURATION
# =============================================================================

# Device for AI inference
# Options: cpu, cuda
DEVICE=cpu

# CPU thread configuration for inference
# 0 means auto-detect/use framework defaults
CPU_THREADS=0

# Voice Activity Detection (reduces hallucinations in faster-whisper)
# Options: true, false
VAD=false

# Clean intermediate files after processing
# Options: true, false
CLEAN_INTERMEDIATE_FILES=false

# =============================================================================
# PERFORMANCE & CACHING SETTINGS
# =============================================================================

# Enable model caching for better performance
# Options: true, false
MODEL_CACHE_ENABLED=true

# Preload models at startup (improves first-request performance)
# Options: true, false
PRELOAD_MODELS=true

# Default model selections (used when preloading)
DEFAULT_STT_MODEL=tiny
DEFAULT_TRANSLATION_MODEL=nllb-200-distilled-600M

# =============================================================================
# DEPLOYMENT ENVIRONMENT EXAMPLES
# =============================================================================

# Development Environment (Fast startup, minimal resources)
# PRELOAD_MODELS=false
# MODEL_CACHE_ENABLED=true
# LOG_LEVEL=DEBUG
# CPU_THREADS=0
# MAX_CONCURRENT_JOBS=1

# Production Environment (Optimal performance)
# PRELOAD_MODELS=true
# MODEL_CACHE_ENABLED=true
# LOG_LEVEL=INFO
# CPU_THREADS=0
# MAX_CONCURRENT_JOBS=4

# Resource-Constrained Environment (Minimal memory usage)
# PRELOAD_MODELS=false
# MODEL_CACHE_ENABLED=false
# LOG_LEVEL=WARNING
# CPU_THREADS=2
# MAX_CONCURRENT_JOBS=1

# =============================================================================
# NOTES
# =============================================================================
# 
# 1. All variables listed above are actually used by the application
# 2. Database settings are currently hardcoded for in-memory SQLite
# 3. File format restrictions are hardcoded (MP4 only)
# 4. AWS configuration is handled separately during deployment
# 
# For deployment-specific configuration, see:
# - docker-compose.yml (Docker deployment)
# - aws/cloudformation-infrastructure.yml (AWS deployment)
# - DEPLOYMENT_GUIDE.md (deployment instructions)