# Open-Dubbing Environment Variables Configuration
# Copy this file to .env and set your actual values

# Required: Hugging Face API token for PyAnnote models
# Get your token from: https://huggingface.co/settings/tokens
# You need to accept the user agreements for segmentation and diarization models
HUGGING_FACE_TOKEN=hf_your_token_here
# Alternative variable name (same as above)
# HF_TOKEN=hf_your_token_here

# Optional: Output directory for processed files
# Default: output/
OUTPUT_DIRECTORY=output/

# Optional: Logging level
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
# Default: INFO
LOG_LEVEL=INFO

# Optional: Device for inference
# Options: cpu, cuda
# Default: cpu
DEVICE=cpu

# Optional: Number of CPU threads for inference
# 0 means auto-detect/use framework defaults
# Default: 0
CPU_THREADS=0

# Optional: Enable VAD (Voice Activity Detection) filter
# Reduces hallucinations when using faster-whisper
# Options: true, false, 1, 0, yes, no, on, off
# Default: false
VAD=false

# Optional: Clean intermediate files after processing
# Options: true, false, 1, 0, yes, no, on, off
# Default: false
CLEAN_INTERMEDIATE_FILES=false

# Required when using OpenAI TTS (--tts openai)
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your_openai_api_key_here

# Required when using custom TTS API (--tts api)
# URL of your custom TTS API server
TTS_API_SERVER=http://localhost:8080

# === Phase 2: Model Management & Performance Settings ===

# Optional: Enable model caching (Phase 2 feature)
# When enabled, models are cached in memory for reuse
# Options: true, false, 1, 0, yes, no, on, off
# Default: true
MODEL_CACHE_ENABLED=true

# Optional: Enable model preloading at startup (Phase 2 feature)
# When enabled, default models are loaded at application startup
# This improves first-request performance but increases startup time
# Options: true, false, 1, 0, yes, no, on, off
# Default: true
PRELOAD_MODELS=true

# === Web Service Configuration ===

# Optional: Server host for FastAPI application
# Default: 0.0.0.0 (all interfaces)
HOST=0.0.0.0

# Optional: Server port for FastAPI application
# Default: 8000
PORT=8000

# === Phase 3: Complete API Implementation ===

# Database Configuration
DATABASE_URL=:memory:               # SQLite in-memory database (Phase 3 default)
# DATABASE_URL=sqlite:///app.db     # Alternative: persistent SQLite file

# Job Queue Configuration
MAX_CONCURRENT_JOBS=2               # Maximum concurrent video processing jobs
JOB_QUEUE_SIZE=100                 # Maximum number of jobs in queue

# WebSocket Configuration
WEBSOCKET_PING_INTERVAL=30          # WebSocket ping interval (seconds)
WEBSOCKET_TIMEOUT=300              # WebSocket connection timeout (seconds)

# File Upload Configuration
MAX_FILE_SIZE_MB=200               # Maximum upload file size in MB
ALLOWED_FILE_FORMATS=mp4           # Comma-separated list of allowed formats

# Storage Configuration (Future S3 Support)
STORAGE_TYPE=local                 # local or s3
UPLOAD_DIRECTORY=uploads/          # Local upload directory

# S3 Configuration (for future use)
# AWS_ACCESS_KEY_ID=your_access_key
# AWS_SECRET_ACCESS_KEY=your_secret_key
# AWS_REGION=us-east-1
# S3_BUCKET_NAME=your-bucket-name

# =============================================================================
# Development vs Production Settings
# =============================================================================

# Development Environment (Fast startup, caching enabled)
# PRELOAD_MODELS=false
# MODEL_CACHE_ENABLED=true
# LOG_LEVEL=DEBUG
# MAX_CONCURRENT_JOBS=1

# Production Environment (Optimal performance)
# PRELOAD_MODELS=true
# MODEL_CACHE_ENABLED=true
# LOG_LEVEL=INFO
# MAX_CONCURRENT_JOBS=4

# Resource-Constrained Environment (Minimal memory usage)
# PRELOAD_MODELS=false
# MODEL_CACHE_ENABLED=false
# MAX_CONCURRENT_JOBS=1
# CPU_THREADS=2